{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Set up automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import CLIPModel\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from dataset import FlickrDataset\n",
    "from decoder import Decoder\n",
    "\n",
    "from utils import DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"nlphuji/flickr30k\", cache_dir=\"./data\")\n",
    "# train_data = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "# test_data = dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "# valid_data = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "# relevant_columns = [\"image\", \"caption\"]\n",
    "# train_dataset = train_data.select_columns(relevant_columns)\n",
    "# test_dataset = test_data.select_columns(relevant_columns)\n",
    "# valid_dataset = valid_data.select_columns(relevant_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset & dataloader\n",
    "dataset = FlickrDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Load the model\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Freeze the CLIP model\n",
    "for param in clip_model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in clip_model.text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize the decoder\n",
    "decoder = Decoder(num_layers=1, embedding_dim=512, num_heads=4, ff_dim=1024).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/243, Loss: 10.6508\n",
      "Epoch 1, Batch 26/243, Loss: 11.7485\n",
      "Epoch 1, Batch 51/243, Loss: 11.1041\n",
      "Epoch 1, Batch 76/243, Loss: 11.2058\n",
      "Epoch 1, Batch 101/243, Loss: 11.0673\n",
      "Epoch 1, Batch 126/243, Loss: 10.5601\n",
      "Epoch 1, Batch 151/243, Loss: 10.4117\n",
      "Epoch 1, Batch 176/243, Loss: 11.1671\n",
      "Epoch 1, Batch 201/243, Loss: 10.9967\n",
      "Epoch 1, Batch 226/243, Loss: 10.6263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# batch = next(iter(dataloader))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     images, captions \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m----> 8\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     11\u001b[0m     clip_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "    # batch = next(iter(dataloader))\n",
    "        images, captions = batch\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "\n",
    "        clip_model.eval()\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(pixel_values=images)\n",
    "            text_features = clip_model.text_model(\n",
    "                input_ids=captions,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden_state = text_features.last_hidden_state  # shape: (batch size, sequence length, hidden size)\n",
    "            # print(last_hidden_state.shape, image_features.shape)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Concatenate the image features and the last hidden state\n",
    "        decoder_input = torch.cat((image_features.unsqueeze(1), last_hidden_state), dim=1)\n",
    "\n",
    "        # Pass the concatenated features to the decoder\n",
    "        logits = decoder(decoder_input)\n",
    "\n",
    "        # Project the decoder output to the vocabulary size\n",
    "        vocab_projection = nn.Linear(512, dataset.vocab_size).to(DEVICE)\n",
    "\n",
    "        # Project the logits to the vocabulary size\n",
    "        logits = vocab_projection(logits)\n",
    "\n",
    "        # Remove the CLS token\n",
    "        logits = logits[:, 1:, :].contiguous()\n",
    "\n",
    "        # Flatten the logits and targets\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = captions.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 25 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} completed, Epoch loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # decoder_input = decoder_input.view(decoder_input.size(0), -1, decoder_input.size(1))\n",
    "        # decoder_input.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "234*7/60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-transfer-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
