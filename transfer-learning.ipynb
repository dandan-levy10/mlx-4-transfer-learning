{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up automatic reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import CLIPModel\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from dataset import FlickrDataset\n",
    "from decoder import Decoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"nlphuji/flickr30k\", cache_dir=\"./data\")\n",
    "# train_data = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "# test_data = dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "# valid_data = dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "# relevant_columns = [\"image\", \"caption\"]\n",
    "# train_dataset = train_data.select_columns(relevant_columns)\n",
    "# test_dataset = test_data.select_columns(relevant_columns)\n",
    "# valid_dataset = valid_data.select_columns(relevant_columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset & dataloader\n",
    "dataset = FlickrDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Freeze the CLIP model\n",
    "for param in clip_model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in clip_model.text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize the decoder\n",
    "decoder = Decoder(num_layers=1, embedding_dim=64, num_heads=4, ff_dim=128).to(DEVICE)\n",
    "\n",
    "# Initialize the vocabulary projection\n",
    "vocab_projection = nn.Linear(64, dataset.vocab_size).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(list(decoder.parameters()) + list(vocab_projection.parameters()), lr=0.001)\n",
    "\n",
    "# Schedule learning rate\n",
    "scheduler = optim.NoamLR(optimizer, d_model=64, factor=1.0, warmup=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/970 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/970, Loss: 10.8284\n",
      "Epoch 1, Batch 26/970, Loss: 4.9279\n",
      "Epoch 1, Batch 51/970, Loss: 2.5473\n",
      "Epoch 1, Batch 76/970, Loss: 1.7243\n",
      "Epoch 1, Batch 101/970, Loss: 1.3994\n",
      "Epoch 1, Batch 126/970, Loss: 1.6091\n",
      "Epoch 1, Batch 151/970, Loss: 1.4810\n",
      "Epoch 1, Batch 176/970, Loss: 1.2351\n",
      "Epoch 1, Batch 201/970, Loss: 1.1416\n",
      "Epoch 1, Batch 226/970, Loss: 1.1932\n",
      "Epoch 1, Batch 251/970, Loss: 1.0030\n",
      "Epoch 1, Batch 276/970, Loss: 1.0143\n",
      "Epoch 1, Batch 301/970, Loss: 0.9610\n",
      "Epoch 1, Batch 326/970, Loss: 0.7091\n",
      "Epoch 1, Batch 351/970, Loss: 0.8895\n",
      "Epoch 1, Batch 376/970, Loss: 0.7167\n",
      "Epoch 1, Batch 401/970, Loss: 0.8291\n",
      "Epoch 1, Batch 426/970, Loss: 0.8426\n",
      "Epoch 1, Batch 451/970, Loss: 0.7285\n",
      "Epoch 1, Batch 476/970, Loss: 0.6392\n",
      "Epoch 1, Batch 501/970, Loss: 0.5643\n",
      "Epoch 1, Batch 526/970, Loss: 0.5267\n",
      "Epoch 1, Batch 551/970, Loss: 0.6886\n",
      "Epoch 1, Batch 576/970, Loss: 0.5561\n",
      "Epoch 1, Batch 601/970, Loss: 0.5511\n",
      "Epoch 1, Batch 626/970, Loss: 0.4790\n",
      "Epoch 1, Batch 651/970, Loss: 0.5710\n",
      "Epoch 1, Batch 676/970, Loss: 0.5830\n",
      "Epoch 1, Batch 701/970, Loss: 0.4865\n",
      "Epoch 1, Batch 726/970, Loss: 0.4416\n",
      "Epoch 1, Batch 751/970, Loss: 0.4971\n",
      "Epoch 1, Batch 776/970, Loss: 0.4650\n",
      "Epoch 1, Batch 801/970, Loss: 0.3987\n",
      "Epoch 1, Batch 826/970, Loss: 0.3838\n",
      "Epoch 1, Batch 851/970, Loss: 0.3902\n",
      "Epoch 1, Batch 876/970, Loss: 0.5105\n",
      "Epoch 1, Batch 901/970, Loss: 0.3368\n",
      "Epoch 1, Batch 926/970, Loss: 0.3089\n",
      "Epoch 1, Batch 951/970, Loss: 0.3036\n",
      "Epoch 1 completed, Epoch loss: 1.0065\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=True)\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "    # batch = next(iter(dataloader))\n",
    "        images, captions = batch\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "\n",
    "        clip_model.eval()\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(pixel_values=images)\n",
    "            text_features = clip_model.text_model(\n",
    "                input_ids=captions,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden_state = text_features.last_hidden_state  # shape: (batch size, sequence length, hidden size)\n",
    "            # print(last_hidden_state.shape, image_features.shape)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Concatenate the image features and the last hidden state\n",
    "        decoder_input = torch.cat((image_features.unsqueeze(1), last_hidden_state), dim=1)\n",
    "\n",
    "        # Pass the concatenated features to the decoder\n",
    "        logits = decoder(decoder_input)\n",
    "\n",
    "        # Project the logits to the vocabulary size\n",
    "        logits = vocab_projection(logits)\n",
    "\n",
    "        # Remove the CLS token\n",
    "        logits = logits[:, 1:, :].contiguous()\n",
    "\n",
    "        # Flatten the logits and targets\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = captions.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if batch_idx % 25 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} completed, Epoch loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "234*7/60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
